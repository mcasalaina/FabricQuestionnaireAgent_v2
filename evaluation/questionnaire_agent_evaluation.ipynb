{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848a073",
   "metadata": {},
   "source": [
    "# Questionnaire Agent Evaluation\n",
    "\n",
    "This notebook evaluates the Questionnaire Agent using a comprehensive set of test queries focused on Azure AI topics.\n",
    "The evaluation includes both Azure AI-specific questions and general questions to test the agent's ability to stay on topic and provide accurate, contextually relevant responses.\n",
    "\n",
    "This evaluation system uses Azure AI evaluation SDK to assess:\n",
    "- **Relevance**: How relevant responses are to the queries\n",
    "- **Coherence**: How logically structured and consistent responses are\n",
    "- **Fluency**: How well-written and readable responses are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03554da",
   "metadata": {},
   "source": [
    "## Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e235d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üìÅ Working directory: c:\\src\\QuestionnaireAgent_v2\\evaluation\n",
      "üìÇ Parent directory added to path: c:\\src\\QuestionnaireAgent_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure AI evaluation imports\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "\n",
    "# Azure authentication and client imports\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Add the parent directory to sys.path to import the questionnaire agent\n",
    "parent_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÇ Parent directory added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded:\n",
      "  Azure OpenAI Endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n",
      "  Main Model Deployment: gpt-4.1\n",
      "  Evaluation Model: gpt-4o-mini\n",
      "  Evaluation Endpoint: https://aipmaker-project-resource.openai.azure.com/\n",
      "  Evaluation API Version: 2024-12-01-preview\n",
      "  Bing Connection ID: aipmakerbingsearch\n",
      "‚úÖ All required environment variables are configured!\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "\n",
    "# Evaluation model configuration (use the same or different model for evaluation)\n",
    "EVALUATION_MODEL_DEPLOYMENT = os.getenv(\"EVALUATION_MODEL_DEPLOYMENT\")\n",
    "EVALUATION_MODEL_ENDPOINT = os.getenv(\"EVALUATION_MODEL_ENDPOINT\")\n",
    "EVALUATION_OPENAI_API_VERSION = os.getenv(\"EVALUATION_OPENAI_API_VERSION\")\n",
    "\n",
    "# Bing Search configuration\n",
    "BING_CONNECTION_ID = os.getenv(\"BING_CONNECTION_ID\")\n",
    "\n",
    "# Application Insights for tracing\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING = os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"  Main Model Deployment: {AZURE_OPENAI_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Model: {EVALUATION_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "print(f\"  Evaluation API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "print(f\"  Bing Connection ID: {BING_CONNECTION_ID}\")\n",
    "\n",
    "# Verify required configurations\n",
    "missing_configs = []\n",
    "if not AZURE_OPENAI_ENDPOINT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_ENDPOINT\")\n",
    "if not AZURE_OPENAI_MODEL_DEPLOYMENT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n",
    "if not BING_CONNECTION_ID:\n",
    "    missing_configs.append(\"BING_CONNECTION_ID\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing_configs)}\")\n",
    "    print(\"   Please check your .env file configuration.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fb67f",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bd41a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Project Client initialized successfully!\n",
      "üîó Connected to endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure AI Project Client for evaluation\n",
    "try:\n",
    "    # Use Azure CLI credentials for authentication\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    # Initialize the Azure AI Project Client\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Azure AI Project Client initialized successfully!\")\n",
    "    print(f\"üîó Connected to endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure AI Project Client: {str(e)}\")\n",
    "    print(\"   Please ensure you are logged in with 'az login' and have proper permissions.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e56b",
   "metadata": {},
   "source": [
    "## Create Questionnaire Agent Query Function\n",
    "\n",
    "This function serves as the target for the evaluation system. It will use the existing questionnaire agent to process queries and return responses in the format expected by the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365818fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported QuestionnaireAgentUI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry._logs._internal:Overriding of current LoggerProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.metrics._internal:Overriding of current MeterProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n",
      "INFO:question_answerer:‚úÖ Azure AI Foundry tracing initialized successfully (content recording: enabled).\n",
      "INFO:question_answerer:Connecting to Azure AI Foundry endpoint: https://aipmaker-project-resource.services.ai.azure.com/api/projects/aipmaker-project\n",
      "INFO:question_answerer:Azure AI Project Client initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Questionnaire agent initialized in headless mode\n"
     ]
    }
   ],
   "source": [
    "# Import the questionnaire agent\n",
    "try:\n",
    "    from question_answerer import QuestionnaireAgentUI\n",
    "    print(\"‚úÖ Successfully imported QuestionnaireAgentUI\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import QuestionnaireAgentUI: {str(e)}\")\n",
    "    print(\"   Make sure the question_answerer.py file is in the parent directory\")\n",
    "    raise\n",
    "\n",
    "# Initialize the questionnaire agent in headless mode for evaluation\n",
    "try:\n",
    "    questionnaire_agent = QuestionnaireAgentUI(\n",
    "        headless_mode=True, \n",
    "        max_retries=3,  # Limit retries for faster evaluation\n",
    "        mock_mode=False  # Use real Azure AI services for evaluation\n",
    "    )\n",
    "    print(\"‚úÖ Questionnaire agent initialized in headless mode\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize questionnaire agent: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856c4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:question_answerer:Agent cleanup completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing the query function with a sample question...\n",
      "‚úÖ Test query: What are the key features of Azure AI?\n",
      "üìù Response preview: Azure AI delivers a comprehensive suite of cloud-based artificial intelligence services designed to help organizations build, deploy, and manage AI solutions efficiently. Its key features include: Azu...\n"
     ]
    }
   ],
   "source": [
    "def query_questionnaire_agent(query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Target function for evaluation that queries the questionnaire agent.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask the agent\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary containing query and response for evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use default context and parameters for evaluation\n",
    "        context = \"Microsoft Azure AI\"\n",
    "        char_limit = 2000\n",
    "        max_retries = 3\n",
    "        verbose = False\n",
    "        \n",
    "        # Process the query using the questionnaire agent\n",
    "        success, answer, links = questionnaire_agent.process_single_question_cli(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            char_limit=char_limit,\n",
    "            verbose=verbose,\n",
    "            max_retries=max_retries\n",
    "        )\n",
    "        \n",
    "        if success and answer:\n",
    "            # Return in the format expected by the evaluation framework\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": answer\n",
    "            }\n",
    "        else:\n",
    "            # Handle case where agent failed to generate a response\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"The agent was unable to generate a response for this query.\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying agent for '{query[:50]}...': {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error occurred while processing query: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Test the function with a sample query\n",
    "print(\"üß™ Testing the query function with a sample question...\")\n",
    "test_result = query_questionnaire_agent(\"What are the key features of Azure AI?\")\n",
    "print(f\"‚úÖ Test query: {test_result['query']}\")\n",
    "print(f\"üìù Response preview: {test_result['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55356cca",
   "metadata": {},
   "source": [
    "## Configure AI Evaluation Models\n",
    "\n",
    "Set up the Azure OpenAI model configuration and initialize the evaluation metrics that will assess the quality of the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model configuration created for evaluation\n",
      "   Endpoint: https://aipmaker-project-resource.openai.azure.com/\n",
      "   Deployment: gpt-4o-mini\n",
      "   API Version: 2024-12-01-preview\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure OpenAI model for evaluation\n",
    "# Note: We need to use API key authentication for the evaluation models\n",
    "# since the evaluation SDK requires explicit API keys\n",
    "\n",
    "# Try to get API key from environment\n",
    "EVALUATION_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "if not EVALUATION_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Warning: No Azure OpenAI API key found in environment variables.\")\n",
    "    print(\"   The evaluation will attempt to use Azure CLI credentials, but may need API key.\")\n",
    "    print(\"   Consider setting AZURE_OPENAI_KEY in your .env file.\")\n",
    "\n",
    "# Configure the evaluation model\n",
    "try:\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=EVALUATION_MODEL_ENDPOINT,\n",
    "        azure_deployment=EVALUATION_MODEL_DEPLOYMENT,\n",
    "        api_version=EVALUATION_OPENAI_API_VERSION,\n",
    "        api_key=EVALUATION_API_KEY\n",
    "    )\n",
    "    print(f\"‚úÖ Model configuration created for evaluation\")\n",
    "    print(f\"   Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "    print(f\"   Deployment: {EVALUATION_MODEL_DEPLOYMENT}\")\n",
    "    print(f\"   API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create model configuration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ca7ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing evaluation metrics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Relevance evaluator initialized\n",
      "‚úÖ Coherence evaluator initialized\n",
      "‚úÖ Fluency evaluator initialized\n",
      "\n",
      "üéØ All evaluators configured successfully!\n",
      "   Each evaluator will assess different aspects of response quality:\n",
      "   ‚Ä¢ Relevance: How relevant responses are to queries\n",
      "   ‚Ä¢ Coherence: How logically structured responses are\n",
      "   ‚Ä¢ Fluency: How well-written and readable responses are\n"
     ]
    }
   ],
   "source": [
    "# Initialize all evaluators\n",
    "try:\n",
    "    print(\"üîß Initializing evaluation metrics...\")\n",
    "    \n",
    "    # Relevance: Measures how relevant the response is to the query\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Relevance evaluator initialized\")\n",
    "    \n",
    "    # Coherence: Measures the logical flow and consistency of the response\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Coherence evaluator initialized\")\n",
    "    \n",
    "    # Fluency: Measures the readability and linguistic quality of the response\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Fluency evaluator initialized\")\n",
    "    \n",
    "    print(\"\\nüéØ All evaluators configured successfully!\")\n",
    "    print(\"   Each evaluator will assess different aspects of response quality:\")\n",
    "    print(\"   ‚Ä¢ Relevance: How relevant responses are to queries\")\n",
    "    print(\"   ‚Ä¢ Coherence: How logically structured responses are\")\n",
    "    print(\"   ‚Ä¢ Fluency: How well-written and readable responses are\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring evaluators: {str(e)}\")\n",
    "    print(\"   This might be due to API key issues or model configuration problems.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d1260",
   "metadata": {},
   "source": [
    "## Run Comprehensive Agent Evaluation\n",
    "\n",
    "Execute the evaluation pipeline using all configured evaluators against the test dataset. This process will measure agent performance across multiple dimensions.\n",
    "\n",
    "**Note**: This evaluation may take a significant amount of time depending on the number of queries and the response time of the agent and evaluation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive evaluation: questionnaire_agent_evaluation_20250904_084903\n",
      "üìä Total queries to evaluate: 3\n",
      "‚è∞ Started at: 2025-09-04 08:49:03\n",
      "\n",
      "‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\n",
      "   Each query requires multiple API calls to the agent and evaluation models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:question_answerer:Error generating answer: (None) Thread thread_xF6tU2KElyjpFSxMRSfj8hGE already has an active run run_on5fvIHo6zAvFRURVq2miXiG.\n",
      "Code: None\n",
      "Message: Thread thread_xF6tU2KElyjpFSxMRSfj8hGE already has an active run run_on5fvIHo6zAvFRURVq2miXiG.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 08:49:18 -0700   59524 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-09-04 08:49:18 -0700   59524 execution.bulk     INFO     Average execution time for completed lines: 9.71 seconds. Estimated time for incomplete lines: 19.42 seconds.\n",
      "2025-09-04 08:49:54 -0700   59524 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-09-04 08:49:54 -0700   59524 execution.bulk     INFO     Average execution time for completed lines: 22.77 seconds. Estimated time for incomplete lines: 22.77 seconds.\n",
      "2025-09-04 08:50:48 -0700   59524 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-09-04 08:50:48 -0700   59524 execution.bulk     INFO     Average execution time for completed lines: 33.37 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"query_questionnaire_agent_20250904_154908_619282\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-09-04 15:49:08.619282+00:00\"\n",
      "Duration: \"0:01:40.930162\"\n",
      "\n",
      "2025-09-04 08:50:56 -0700   31324 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   63812 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   63812 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700    6008 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   63812 execution.bulk     INFO     Average execution time for completed lines: 7.11 seconds. Estimated time for incomplete lines: 14.22 seconds.\n",
      "2025-09-04 08:50:56 -0700   31324 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700    6008 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700   63812 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   31324 execution.bulk     INFO     Average execution time for completed lines: 7.1 seconds. Estimated time for incomplete lines: 14.2 seconds.\n",
      "2025-09-04 08:50:56 -0700    6008 execution.bulk     INFO     Average execution time for completed lines: 7.08 seconds. Estimated time for incomplete lines: 14.16 seconds.\n",
      "2025-09-04 08:50:56 -0700   63812 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   31324 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   63812 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700    6008 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   63812 execution.bulk     INFO     Average execution time for completed lines: 2.39 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-09-04 08:50:56 -0700   31324 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700    6008 azure.ai.evaluation._legacy.prompty._prompty ERROR    [0/10] AsyncAzureOpenAI request failed. AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\prompty\\_prompty.py\", line 382, in _send_with_retries\n",
      "    response = await client.chat.completions.create(**params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\azure\\ai\\evaluation\\_legacy\\_batch_engine\\_openai_injector.py\", line 50, in async_wrapper\n",
      "    result: _WithUsage = await method(*args, **kwargs)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2583, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<48 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\src\\QuestionnaireAgent_v2\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1594, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}}\n",
      "2025-09-04 08:50:56 -0700   31324 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700    6008 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-09-04 08:50:56 -0700   31324 execution.bulk     INFO     Average execution time for completed lines: 2.39 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-09-04 08:50:56 -0700    6008 execution.bulk     INFO     Average execution time for completed lines: 2.38 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-09-04 08:50:56 -0700   31324 execution          ERROR    3/3 flow run failed, indexes: [0,1,2], exception of index 0: Error while evaluating single input: WrappedOpenAIError: (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2025-09-04 08:50:56 -0700   63812 execution          ERROR    3/3 flow run failed, indexes: [0,1,2], exception of index 0: Error while evaluating single input: WrappedOpenAIError: (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:run:Run relevance_20250904_155049_648929 failed with status 4.\n",
      "Error: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20250904_155049_648929\"\n",
      "Run status: \"Failed\"\n",
      "Start time: \"2025-09-04 15:50:49.648929+00:00\"\n",
      "Duration: \"0:00:07.290247\"\n",
      "\n",
      "2025-09-04 08:50:56 -0700    6008 execution          ERROR    3/3 flow run failed, indexes: [0,1,2], exception of index 0: Error while evaluating single input: WrappedOpenAIError: (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "azure.ai.evaluation._legacy._batch_engine._exceptions.BatchEngineRunFailedError: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:run:Run coherence_20250904_155049_614785 failed with status 4.\n",
      "Error: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"coherence_20250904_155049_614785\"\n",
      "Run status: \"Failed\"\n",
      "Start time: \"2025-09-04 15:50:49.614785+00:00\"\n",
      "Duration: \"0:00:07.329933\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:run:Run fluency_20250904_155049_665242 failed with status 4.\n",
      "Error: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"fluency_20250904_155049_665242\"\n",
      "Run status: \"Failed\"\n",
      "Start time: \"2025-09-04 15:50:49.665242+00:00\"\n",
      "Duration: \"0:00:07.282080\"\n",
      "\n",
      "azure.ai.evaluation._legacy._batch_engine._exceptions.BatchEngineRunFailedError: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "azure.ai.evaluation._legacy._batch_engine._exceptions.BatchEngineRunFailedError: (InternalError) 100% of the batch run failed. (UserError) OpenAI API hits AuthenticationError: Error code: 401 - {'error': {'code': 'Unauthorized', 'message': 'Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:run:Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:run:Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "WARNING:run:Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Failed\",\n",
      "        \"duration\": \"0:00:07.290247\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 3,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"coherence\": {\n",
      "        \"status\": \"Failed\",\n",
      "        \"duration\": \"0:00:07.329933\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 3,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Failed\",\n",
      "        \"duration\": \"0:00:07.282080\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 3,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "\n",
      "‚úÖ Evaluation completed successfully!\n",
      "‚è∞ Finished at: 2025-09-04 08:51:09\n",
      "üîó Azure AI Foundry Studio URL: https://ai.azure.com/resource/build/evaluation/fa1e87c3-53a8-4b4d-b1ca-bab46d35b9de?wsid=/subscriptions/21039746-6e73-4627-88af-efa80f856c2c/resourceGroups/rg-AIPMaker/providers/Microsoft.CognitiveServices/accounts/aipmaker-project-resource/projects/aipmaker-project&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "   You can view detailed results and visualizations in the Azure AI Foundry portal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä EVALUATION RESULTS SUMMARY\n",
      "==================================================\n",
      "‚ö†Ô∏è  No metrics found in evaluation results\n",
      "   This might indicate an issue with the evaluation process\n",
      "üíæ Saving detailed results to: evaluation_results\\evaluation_results_20250904_085110.json\n",
      "‚ùå Error exporting results: name 'EVALUATION_MODEL_DEPLOYMENT' is not defined\n",
      "   Results are still available in the evaluation_result variable\n"
     ]
    }
   ],
   "source": [
    "# Prepare for evaluation\n",
    "evaluation_name = f\"questionnaire_agent_evaluation_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üöÄ Starting comprehensive evaluation: {evaluation_name}\")\n",
    "print(f\"üìä Total queries to evaluate: {len(queries)}\")\n",
    "print(f\"‚è∞ Started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\")\n",
    "print(\"   Each query requires multiple API calls to the agent and evaluation models.\")\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    # Configure the evaluators dictionary with proper naming for Azure AI Foundry\n",
    "    evaluators_config = {\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "    }\n",
    "    \n",
    "    # Execute the evaluation\n",
    "    evaluation_result = evaluate(\n",
    "        data=\"small_evaluation_queries.jsonl\",\n",
    "        target=query_questionnaire_agent,\n",
    "        evaluators=evaluators_config,\n",
    "        azure_ai_project=AZURE_OPENAI_ENDPOINT,  # Azure AI project endpoint\n",
    "        evaluation_name=evaluation_name,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"‚è∞ Finished at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Display Azure AI Foundry Studio URL if available\n",
    "    if 'studio_url' in evaluation_result:\n",
    "        print(f\"üîó Azure AI Foundry Studio URL: {evaluation_result['studio_url']}\")\n",
    "        print(\"   You can view detailed results and visualizations in the Azure AI Foundry portal.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   ‚Ä¢ Ensure Azure CLI authentication is working: 'az login'\")\n",
    "    print(\"   ‚Ä¢ Check that all required environment variables are set\")\n",
    "    print(\"   ‚Ä¢ Verify Azure OpenAI API quotas and limits\")\n",
    "    print(\"   ‚Ä¢ Check network connectivity to Azure services\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54140a",
   "metadata": {},
   "source": [
    "## Process and Display Evaluation Results\n",
    "\n",
    "Extract and format evaluation metrics, display summary statistics, and provide detailed analysis of agent performance across different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a636d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display evaluation metrics\n",
    "print(\"üìä EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get the overall metrics\n",
    "    metrics = evaluation_result.get(\"metrics\", {})\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"‚ö†Ô∏è  No metrics found in evaluation results\")\n",
    "        print(\"   This might indicate an issue with the evaluation process\")\n",
    "    else:\n",
    "        print(\"\\nüéØ Overall Performance Metrics:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            # Format the metric value based on its type\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value}\")\n",
    "        \n",
    "        # Calculate and display average score\n",
    "        numeric_metrics = {k: v for k, v in metrics.items() if isinstance(v, (int, float))}\n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            print(f\"\\nüìà Average Score Across All Metrics: {avg_score:.4f}\")\n",
    "            \n",
    "            # Provide interpretation\n",
    "            print(f\"\\nüìù Performance Interpretation:\")\n",
    "            if avg_score >= 4.0:\n",
    "                print(\"   üü¢ Excellent: The agent demonstrates high-quality responses\")\n",
    "            elif avg_score >= 3.0:\n",
    "                print(\"   üü° Good: The agent performs well with room for improvement\")\n",
    "            elif avg_score >= 2.0:\n",
    "                print(\"   üü† Fair: The agent shows moderate performance, needs improvement\")\n",
    "            else:\n",
    "                print(\"   üî¥ Poor: The agent requires significant improvements\")\n",
    "        \n",
    "        # Analyze individual metrics\n",
    "        print(f\"\\nüîç Detailed Metric Analysis:\")\n",
    "        for metric_name, metric_value in numeric_metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                if metric_value >= 4.0:\n",
    "                    status = \"üü¢ Excellent\"\n",
    "                elif metric_value >= 3.0:\n",
    "                    status = \"üü° Good\"\n",
    "                elif metric_value >= 2.0:\n",
    "                    status = \"üü† Fair\"\n",
    "                else:\n",
    "                    status = \"üî¥ Poor\"\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f} - {status}\")\n",
    "    \n",
    "    # Display additional result information\n",
    "    if 'outputs' in evaluation_result:\n",
    "        outputs = evaluation_result['outputs']\n",
    "        print(f\"\\nüìã Detailed Results Available:\")\n",
    "        print(f\"   ‚Ä¢ Number of evaluated samples: {len(outputs) if hasattr(outputs, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    # Display data info if available\n",
    "    if 'data' in evaluation_result:\n",
    "        print(f\"\\nüìÅ Evaluation Data Info:\")\n",
    "        print(f\"   ‚Ä¢ Data source: {evaluation_queries_file}\")\n",
    "        print(f\"   ‚Ä¢ Total queries processed: {len(queries)}\")\n",
    "        print(f\"   ‚Ä¢ Azure AI queries: {len(azure_ai_queries)}\")\n",
    "        print(f\"   ‚Ä¢ General queries: {len(general_queries)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing evaluation results: {str(e)}\")\n",
    "    print(\"   Raw evaluation result keys:\", list(evaluation_result.keys()) if evaluation_result else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a51bf",
   "metadata": {},
   "source": [
    "## Export Results for Analysis\n",
    "\n",
    "Save evaluation results to files, generate reports, and provide information for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41697c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation results to files\n",
    "results_dir = Path(\"evaluation_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "summary_file = results_dir / f\"evaluation_summary_{timestamp}.txt\"\n",
    "\n",
    "try:\n",
    "    # Save detailed results as JSON\n",
    "    print(f\"üíæ Saving detailed results to: {results_file}\")\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"evaluation_name\": evaluation_name,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"configuration\": {\n",
    "                \"model_deployment\": EVALUATION_MODEL_DEPLOYMENT,\n",
    "                \"evaluation_model\": EVALUATION_MODEL_DEPLOYMENT,\n",
    "                \"total_queries\": len(queries),\n",
    "                \"azure_ai_queries\": len(azure_ai_queries),\n",
    "                \"general_queries\": len(general_queries)\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"evaluation_result\": evaluation_result\n",
    "        }, f, indent=2, default=str)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"üìÑ Generating summary report: {summary_file}\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"QUESTIONNAIRE AGENT EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Evaluation Name: {evaluation_name}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Queries Evaluated: {len(queries)}\\n\")\n",
    "        f.write(f\"Azure AI Related Queries: {len(azure_ai_queries)}\\n\")\n",
    "        f.write(f\"General/Off-topic Queries: {len(general_queries)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                f.write(f\"{metric_name.title()}: {metric_value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric_name.title()}: {metric_value}\\n\")\n",
    "        \n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            f.write(f\"\\nAverage Score: {avg_score:.4f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nPerformance Level: \")\n",
    "            if avg_score >= 4.0:\n",
    "                f.write(\"Excellent\\n\")\n",
    "            elif avg_score >= 3.0:\n",
    "                f.write(\"Good\\n\")\n",
    "            elif avg_score >= 2.0:\n",
    "                f.write(\"Fair\\n\")\n",
    "            else:\n",
    "                f.write(\"Poor\\n\")\n",
    "        \n",
    "        if 'studio_url' in evaluation_result:\n",
    "            f.write(f\"\\nAzure AI Foundry Studio URL:\\n{evaluation_result['studio_url']}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Results exported successfully!\")\n",
    "    print(f\"üìÅ Results directory: {results_dir.absolute()}\")\n",
    "    print(f\"   ‚Ä¢ Detailed JSON: {results_file.name}\")\n",
    "    print(f\"   ‚Ä¢ Summary report: {summary_file.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting results: {str(e)}\")\n",
    "    print(\"   Results are still available in the evaluation_result variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41476",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This evaluation notebook provides a comprehensive assessment of the Questionnaire Agent's performance across multiple dimensions:\n",
    "\n",
    "### What This Evaluation Measures:\n",
    "\n",
    "1. **Relevance**: How relevant and on-topic the responses are to the input queries\n",
    "2. **Coherence**: How logically structured and internally consistent the responses are\n",
    "3. **Fluency**: How well-written, readable, and linguistically sound the responses are\n",
    "\n",
    "### Evaluation Dataset:\n",
    "\n",
    "- **Azure AI Queries**: Tests the agent's knowledge and accuracy on its primary domain\n",
    "- **General Queries**: Tests the agent's ability to stay on topic and handle off-domain questions appropriately\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Objective Assessment**: Uses AI-assisted evaluation for consistent, scalable assessment\n",
    "- **Multi-dimensional Analysis**: Evaluates different aspects of response quality\n",
    "- **Azure Integration**: Results are available in Azure AI Foundry Studio for detailed analysis\n",
    "- **Reproducible**: Can be run repeatedly to track improvements over time\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review Detailed Results**: Use the Azure AI Foundry Studio URL to explore individual query results\n",
    "2. **Identify Improvement Areas**: Focus on metrics with lower scores\n",
    "3. **Iterative Development**: Re-run evaluation after making improvements to track progress\n",
    "4. **Expand Evaluation**: Consider adding more specific test cases or custom evaluators\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- Detailed JSON results for programmatic analysis\n",
    "- Human-readable summary reports\n",
    "- Links to Azure AI Foundry Studio for interactive exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
