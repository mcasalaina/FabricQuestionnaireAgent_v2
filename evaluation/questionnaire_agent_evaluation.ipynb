{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848a073",
   "metadata": {},
   "source": [
    "# Questionnaire Agent Evaluation\n",
    "\n",
    "This notebook evaluates the Questionnaire Agent using a comprehensive set of test queries focused on Azure AI topics.\n",
    "The evaluation includes both Azure AI-specific questions and general questions to test the agent's ability to stay on topic and provide accurate, contextually relevant responses.\n",
    "\n",
    "This evaluation system uses Azure AI evaluation SDK to assess:\n",
    "- **Relevance**: How relevant responses are to the queries\n",
    "- **Coherence**: How logically structured and consistent responses are\n",
    "- **Fluency**: How well-written and readable responses are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03554da",
   "metadata": {},
   "source": [
    "## Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e235d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure AI evaluation imports\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "\n",
    "# Azure authentication and client imports\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Add the parent directory to sys.path to import the questionnaire agent\n",
    "parent_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÇ Parent directory added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "\n",
    "# Evaluation model configuration (use the same or different model for evaluation)\n",
    "EVALUATION_MODEL_DEPLOYMENT = os.getenv(\"EVALUATION_MODEL_DEPLOYMENT\")\n",
    "EVALUATION_MODEL_ENDPOINT = os.getenv(\"EVALUATION_MODEL_ENDPOINT\")\n",
    "EVALUATION_OPENAI_API_VERSION = os.getenv(\"EVALUATION_OPENAI_API_VERSION\")\n",
    "\n",
    "# Bing Search configuration\n",
    "BING_CONNECTION_ID = os.getenv(\"BING_CONNECTION_ID\")\n",
    "\n",
    "# Application Insights for tracing\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING = os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"  Main Model Deployment: {AZURE_OPENAI_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Model: {EVALUATION_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "print(f\"  Evaluation API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "print(f\"  Bing Connection ID: {BING_CONNECTION_ID}\")\n",
    "\n",
    "# Verify required configurations\n",
    "missing_configs = []\n",
    "if not AZURE_OPENAI_ENDPOINT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_ENDPOINT\")\n",
    "if not AZURE_OPENAI_MODEL_DEPLOYMENT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n",
    "if not BING_CONNECTION_ID:\n",
    "    missing_configs.append(\"BING_CONNECTION_ID\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing_configs)}\")\n",
    "    print(\"   Please check your .env file configuration.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fb67f",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI Project Client for evaluation\n",
    "try:\n",
    "    # Use Azure CLI credentials for authentication\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    # Initialize the Azure AI Project Client\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Azure AI Project Client initialized successfully!\")\n",
    "    print(f\"üîó Connected to endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure AI Project Client: {str(e)}\")\n",
    "    print(\"   Please ensure you are logged in with 'az login' and have proper permissions.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e56b",
   "metadata": {},
   "source": [
    "## Create Questionnaire Agent Query Function\n",
    "\n",
    "This function serves as the target for the evaluation system. It will use the existing questionnaire agent to process queries and return responses in the format expected by the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365818fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.monitor.opentelemetry.exporter.export._base:Retrying due to server request error: <urllib3.connection.HTTPSConnection object at 0x00000264F266F750>: Failed to resolve 'eastus2-3.in.applicationinsights.azure.com' ([Errno 11001] getaddrinfo failed).\n",
      "WARNING:azure.monitor.opentelemetry.exporter.export._base:Retrying due to server request error: <urllib3.connection.HTTPSConnection object at 0x00000264F266DE50>: Failed to resolve 'eastus2-3.in.applicationinsights.azure.com' ([Errno 11001] getaddrinfo failed).\n",
      "WARNING:azure.monitor.opentelemetry.exporter.export._base:Retrying due to server request error: <urllib3.connection.HTTPSConnection object at 0x00000264F266C7D0>: Failed to resolve 'eastus2-3.in.applicationinsights.azure.com' ([Errno 11001] getaddrinfo failed).\n"
     ]
    }
   ],
   "source": [
    "# Import the questionnaire agent\n",
    "try:\n",
    "    from question_answerer import QuestionnaireAgentUI\n",
    "    print(\"‚úÖ Successfully imported QuestionnaireAgentUI\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import QuestionnaireAgentUI: {str(e)}\")\n",
    "    print(\"   Make sure the question_answerer.py file is in the parent directory\")\n",
    "    raise\n",
    "\n",
    "# Initialize the questionnaire agent in headless mode for evaluation\n",
    "try:\n",
    "    questionnaire_agent = QuestionnaireAgentUI(\n",
    "        headless_mode=True, \n",
    "        max_retries=3,  # Limit retries for faster evaluation\n",
    "        mock_mode=False  # Use real Azure AI services for evaluation\n",
    "    )\n",
    "    print(\"‚úÖ Questionnaire agent initialized in headless mode\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize questionnaire agent: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_questionnaire_agent(query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Target function for evaluation that queries the questionnaire agent.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask the agent\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary containing query and response for evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use default context and parameters for evaluation\n",
    "        context = \"Microsoft Azure AI\"\n",
    "        char_limit = 2000\n",
    "        max_retries = 3\n",
    "        verbose = False\n",
    "        \n",
    "        # Process the query using the questionnaire agent\n",
    "        success, answer, links = questionnaire_agent.process_single_question_cli(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            char_limit=char_limit,\n",
    "            verbose=verbose,\n",
    "            max_retries=max_retries\n",
    "        )\n",
    "        \n",
    "        if success and answer:\n",
    "            # Return in the format expected by the evaluation framework\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": answer\n",
    "            }\n",
    "        else:\n",
    "            # Handle case where agent failed to generate a response\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"The agent was unable to generate a response for this query.\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying agent for '{query[:50]}...': {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error occurred while processing query: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Test the function with a sample query\n",
    "print(\"üß™ Testing the query function with a sample question...\")\n",
    "test_result = query_questionnaire_agent(\"What are the key features of Azure AI?\")\n",
    "print(f\"‚úÖ Test query: {test_result['query']}\")\n",
    "print(f\"üìù Response preview: {test_result['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55356cca",
   "metadata": {},
   "source": [
    "## Configure AI Evaluation Models\n",
    "\n",
    "Set up the Azure OpenAI model configuration and initialize the evaluation metrics that will assess the quality of the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI model for evaluation\n",
    "# Note: We need to use API key authentication for the evaluation models\n",
    "# since the evaluation SDK requires explicit API keys\n",
    "\n",
    "# Try to get API key from environment - check multiple possible variable names\n",
    "EVALUATION_API_KEY = os.getenv(\"EVALUATION_API_KEY\")\n",
    "\n",
    "if not EVALUATION_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Warning: No Azure OpenAI API key found in environment variables.\")\n",
    "    print(\"   Please set EVALUATION_API_KEY in your .env file.\")\n",
    "    print(\"   The evaluation SDK requires explicit API key authentication.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found API key for evaluation (length: {len(EVALUATION_API_KEY)} characters)\")\n",
    "\n",
    "# Configure the evaluation model\n",
    "try:\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=EVALUATION_MODEL_ENDPOINT,\n",
    "        azure_deployment=EVALUATION_MODEL_DEPLOYMENT,\n",
    "        api_version=EVALUATION_OPENAI_API_VERSION,\n",
    "        api_key=EVALUATION_API_KEY\n",
    "    )\n",
    "    print(f\"‚úÖ Model configuration created for evaluation\")\n",
    "    print(f\"   Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "    print(f\"   Deployment: {EVALUATION_MODEL_DEPLOYMENT}\")\n",
    "    print(f\"   API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create model configuration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all evaluators\n",
    "try:\n",
    "    print(\"üîß Initializing evaluation metrics...\")\n",
    "    \n",
    "    # Relevance: Measures how relevant the response is to the query\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Relevance evaluator initialized\")\n",
    "    \n",
    "    # Coherence: Measures the logical flow and consistency of the response\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Coherence evaluator initialized\")\n",
    "    \n",
    "    # Fluency: Measures the readability and linguistic quality of the response\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Fluency evaluator initialized\")\n",
    "    \n",
    "    print(\"\\nüéØ All evaluators configured successfully!\")\n",
    "    print(\"   Each evaluator will assess different aspects of response quality:\")\n",
    "    print(\"   ‚Ä¢ Relevance: How relevant responses are to queries\")\n",
    "    print(\"   ‚Ä¢ Coherence: How logically structured responses are\")\n",
    "    print(\"   ‚Ä¢ Fluency: How well-written and readable responses are\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring evaluators: {str(e)}\")\n",
    "    print(\"   This might be due to API key issues or model configuration problems.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d1260",
   "metadata": {},
   "source": [
    "## Run Comprehensive Agent Evaluation\n",
    "\n",
    "Execute the evaluation pipeline using all configured evaluators against the test dataset. This process will measure agent performance across multiple dimensions.\n",
    "\n",
    "**Note**: This evaluation may take a significant amount of time depending on the number of queries and the response time of the agent and evaluation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for evaluation\n",
    "evaluation_name = f\"questionnaire_agent_evaluation_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üöÄ Starting comprehensive evaluation: {evaluation_name}\")\n",
    "print(f\"‚è∞ Started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\")\n",
    "print(\"   Each query requires multiple API calls to the agent and evaluation models.\")\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    # Configure the evaluators dictionary with proper naming for Azure AI Foundry\n",
    "    evaluators_config = {\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "    }\n",
    "    \n",
    "    # Execute the evaluation\n",
    "    evaluation_result = evaluate(\n",
    "        data=\"evaluation_queries.jsonl\",\n",
    "        target=query_questionnaire_agent,\n",
    "        evaluators=evaluators_config,\n",
    "        azure_ai_project=AZURE_OPENAI_ENDPOINT,  # Azure AI project endpoint\n",
    "        evaluation_name=evaluation_name,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"‚è∞ Finished at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Display Azure AI Foundry Studio URL if available\n",
    "    if 'studio_url' in evaluation_result:\n",
    "        print(f\"üîó Azure AI Foundry Studio URL: {evaluation_result['studio_url']}\")\n",
    "        print(\"   You can view detailed results and visualizations in the Azure AI Foundry portal.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   ‚Ä¢ Ensure Azure CLI authentication is working: 'az login'\")\n",
    "    print(\"   ‚Ä¢ Check that all required environment variables are set\")\n",
    "    print(\"   ‚Ä¢ Verify Azure OpenAI API quotas and limits\")\n",
    "    print(\"   ‚Ä¢ Check network connectivity to Azure services\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41476",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This evaluation notebook provides a comprehensive assessment of the Questionnaire Agent's performance across multiple dimensions:\n",
    "\n",
    "### What This Evaluation Measures:\n",
    "\n",
    "1. **Relevance**: How relevant and on-topic the responses are to the input queries\n",
    "2. **Coherence**: How logically structured and internally consistent the responses are\n",
    "3. **Fluency**: How well-written, readable, and linguistically sound the responses are\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Objective Assessment**: Uses AI-assisted evaluation for consistent, scalable assessment\n",
    "- **Multi-dimensional Analysis**: Evaluates different aspects of response quality\n",
    "- **Azure Integration**: Results are available in Azure AI Foundry Studio for detailed analysis\n",
    "- **Reproducible**: Can be run repeatedly to track improvements over time\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review Detailed Results**: Use the Azure AI Foundry Studio URL to explore individual query results\n",
    "2. **Identify Improvement Areas**: Focus on metrics with lower scores\n",
    "3. **Iterative Development**: Re-run evaluation after making improvements to track progress\n",
    "4. **Expand Evaluation**: Consider adding more specific test cases or custom evaluators\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- Detailed JSON results for programmatic analysis\n",
    "- Human-readable summary reports\n",
    "- Links to Azure AI Foundry Studio for interactive exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
