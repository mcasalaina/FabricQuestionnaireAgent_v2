{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9848a073",
   "metadata": {},
   "source": [
    "# Questionnaire Agent Evaluation\n",
    "\n",
    "This notebook evaluates the Questionnaire Agent using a comprehensive set of test queries focused on Azure AI topics.\n",
    "The evaluation includes both Azure AI-specific questions and general questions to test the agent's ability to stay on topic and provide accurate, contextually relevant responses.\n",
    "\n",
    "This evaluation system uses Azure AI evaluation SDK to assess:\n",
    "- **Groundedness**: How well responses are based on source context\n",
    "- **Relevance**: How relevant responses are to the queries\n",
    "- **Coherence**: How logically structured and consistent responses are\n",
    "- **Fluency**: How well-written and readable responses are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03554da",
   "metadata": {},
   "source": [
    "## Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e235d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure AI evaluation imports\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    GroundednessEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    AzureOpenAIModelConfiguration\n",
    ")\n",
    "\n",
    "# Azure authentication and client imports\n",
    "from azure.identity import AzureCliCredential, DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Add the parent directory to sys.path to import the questionnaire agent\n",
    "parent_dir = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "print(f\"üìÇ Parent directory added to path: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from environment variables\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_MODEL_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\", \"gpt-4o-mini\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-01-01-preview\")\n",
    "\n",
    "# Evaluation model configuration (use the same or different model for evaluation)\n",
    "EVALUATION_MODEL = os.getenv(\"EVALUATION_MODEL\", AZURE_OPENAI_MODEL_DEPLOYMENT)\n",
    "EVALUATION_MODEL_ENDPOINT = os.getenv(\"EVALUATION_MODEL_ENDPOINT\", AZURE_OPENAI_ENDPOINT)\n",
    "EVALUATION_OPENAI_API_VERSION = os.getenv(\"EVALUATION_OPENAI_API_VERSION\", AZURE_OPENAI_API_VERSION)\n",
    "\n",
    "# Bing Search configuration\n",
    "BING_CONNECTION_ID = os.getenv(\"BING_CONNECTION_ID\")\n",
    "\n",
    "# Application Insights for tracing\n",
    "APPLICATIONINSIGHTS_CONNECTION_STRING = os.getenv(\"APPLICATIONINSIGHTS_CONNECTION_STRING\")\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"  Main Model Deployment: {AZURE_OPENAI_MODEL_DEPLOYMENT}\")\n",
    "print(f\"  Evaluation Model: {EVALUATION_MODEL}\")\n",
    "print(f\"  Evaluation Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "print(f\"  API Version: {AZURE_OPENAI_API_VERSION}\")\n",
    "print(f\"  Bing Connection ID: {BING_CONNECTION_ID}\")\n",
    "\n",
    "# Verify required configurations\n",
    "missing_configs = []\n",
    "if not AZURE_OPENAI_ENDPOINT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_ENDPOINT\")\n",
    "if not AZURE_OPENAI_MODEL_DEPLOYMENT:\n",
    "    missing_configs.append(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n",
    "if not BING_CONNECTION_ID:\n",
    "    missing_configs.append(\"BING_CONNECTION_ID\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing_configs)}\")\n",
    "    print(\"   Please check your .env file configuration.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fb67f",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI Project Client for evaluation\n",
    "try:\n",
    "    # Use Azure CLI credentials for authentication\n",
    "    credential = AzureCliCredential()\n",
    "    \n",
    "    # Initialize the Azure AI Project Client\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Azure AI Project Client initialized successfully!\")\n",
    "    print(f\"üîó Connected to endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Azure AI Project Client: {str(e)}\")\n",
    "    print(\"   Please ensure you are logged in with 'az login' and have proper permissions.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4e56b",
   "metadata": {},
   "source": [
    "## Create Questionnaire Agent Query Function\n",
    "\n",
    "This function serves as the target for the evaluation system. It will use the existing questionnaire agent to process queries and return responses in the format expected by the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365818fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the questionnaire agent\n",
    "try:\n",
    "    from question_answerer import QuestionnaireAgentUI\n",
    "    print(\"‚úÖ Successfully imported QuestionnaireAgentUI\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import QuestionnaireAgentUI: {str(e)}\")\n",
    "    print(\"   Make sure the question_answerer.py file is in the parent directory\")\n",
    "    raise\n",
    "\n",
    "# Initialize the questionnaire agent in headless mode for evaluation\n",
    "try:\n",
    "    questionnaire_agent = QuestionnaireAgentUI(\n",
    "        headless_mode=True, \n",
    "        max_retries=3,  # Limit retries for faster evaluation\n",
    "        mock_mode=False  # Use real Azure AI services for evaluation\n",
    "    )\n",
    "    print(\"‚úÖ Questionnaire agent initialized in headless mode\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize questionnaire agent: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_questionnaire_agent(query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Target function for evaluation that queries the questionnaire agent.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask the agent\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary containing query, response, and context for evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use default context and parameters for evaluation\n",
    "        context = \"Microsoft Azure AI\"\n",
    "        char_limit = 2000\n",
    "        max_retries = 3\n",
    "        verbose = False\n",
    "        \n",
    "        # Process the query using the questionnaire agent\n",
    "        success, answer, links = questionnaire_agent.process_single_question_cli(\n",
    "            question=query,\n",
    "            context=context,\n",
    "            char_limit=char_limit,\n",
    "            verbose=verbose,\n",
    "            max_retries=max_retries\n",
    "        )\n",
    "        \n",
    "        if success and answer:\n",
    "            # Return in the format expected by the evaluation framework\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": answer,\n",
    "                \"context\": f\"Question processed in context: {context}. Links found: {', '.join(links) if links else 'None'}\"\n",
    "            }\n",
    "        else:\n",
    "            # Handle case where agent failed to generate a response\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"The agent was unable to generate a response for this query.\",\n",
    "                \"context\": f\"Failed to process question in context: {context}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error querying agent for '{query[:50]}...': {str(e)}\")\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error occurred while processing query: {str(e)}\",\n",
    "            \"context\": \"Error context - agent encountered an exception\"\n",
    "        }\n",
    "\n",
    "# Test the function with a sample query\n",
    "print(\"üß™ Testing the query function with a sample question...\")\n",
    "test_result = query_questionnaire_agent(\"What are the key features of Azure AI?\")\n",
    "print(f\"‚úÖ Test query: {test_result['query']}\")\n",
    "print(f\"üìù Response preview: {test_result['response'][:200]}...\")\n",
    "print(f\"üìã Context: {test_result['context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55356cca",
   "metadata": {},
   "source": [
    "## Configure AI Evaluation Models\n",
    "\n",
    "Set up the Azure OpenAI model configuration and initialize the evaluation metrics that will assess the quality of the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI model for evaluation\n",
    "# Note: We need to use API key authentication for the evaluation models\n",
    "# since the evaluation SDK requires explicit API keys\n",
    "\n",
    "# Try to get API key from environment\n",
    "EVALUATION_API_KEY = os.getenv(\"AZURE_OPENAI_KEY\") or os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "if not EVALUATION_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  Warning: No Azure OpenAI API key found in environment variables.\")\n",
    "    print(\"   The evaluation will attempt to use Azure CLI credentials, but may need API key.\")\n",
    "    print(\"   Consider setting AZURE_OPENAI_KEY in your .env file.\")\n",
    "\n",
    "# Configure the evaluation model\n",
    "try:\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=EVALUATION_MODEL_ENDPOINT,\n",
    "        azure_deployment=EVALUATION_MODEL,\n",
    "        api_version=EVALUATION_OPENAI_API_VERSION,\n",
    "        api_key=EVALUATION_API_KEY\n",
    "    )\n",
    "    print(f\"‚úÖ Model configuration created for evaluation\")\n",
    "    print(f\"   Endpoint: {EVALUATION_MODEL_ENDPOINT}\")\n",
    "    print(f\"   Deployment: {EVALUATION_MODEL}\")\n",
    "    print(f\"   API Version: {EVALUATION_OPENAI_API_VERSION}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create model configuration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all evaluators\n",
    "try:\n",
    "    print(\"üîß Initializing evaluation metrics...\")\n",
    "    \n",
    "    # Groundedness: Measures how well the response is grounded in the provided context\n",
    "    groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Groundedness evaluator initialized\")\n",
    "    \n",
    "    # Relevance: Measures how relevant the response is to the query\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Relevance evaluator initialized\")\n",
    "    \n",
    "    # Coherence: Measures the logical flow and consistency of the response\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Coherence evaluator initialized\")\n",
    "    \n",
    "    # Fluency: Measures the readability and linguistic quality of the response\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"‚úÖ Fluency evaluator initialized\")\n",
    "    \n",
    "    print(\"\\nüéØ All evaluators configured successfully!\")\n",
    "    print(\"   Each evaluator will assess different aspects of response quality:\")\n",
    "    print(\"   ‚Ä¢ Groundedness: How well responses are based on context\")\n",
    "    print(\"   ‚Ä¢ Relevance: How relevant responses are to queries\")\n",
    "    print(\"   ‚Ä¢ Coherence: How logically structured responses are\")\n",
    "    print(\"   ‚Ä¢ Fluency: How well-written and readable responses are\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring evaluators: {str(e)}\")\n",
    "    print(\"   This might be due to API key issues or model configuration problems.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65040bf1",
   "metadata": {},
   "source": [
    "## Load Test Queries from JSONL File\n",
    "\n",
    "Load the evaluation queries that include both Azure AI-specific questions and general questions to comprehensively test the agent's capabilities and topic adherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation queries from JSONL file\n",
    "evaluation_queries_file = \"evaluation_queries.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    if not Path(evaluation_queries_file).exists():\n",
    "        print(f\"‚ùå Evaluation queries file not found: {evaluation_queries_file}\")\n",
    "        print(f\"   Current working directory: {Path.cwd()}\")\n",
    "        print(f\"   Looking for file at: {Path(evaluation_queries_file).absolute()}\")\n",
    "        raise FileNotFoundError(f\"Could not find {evaluation_queries_file}\")\n",
    "    \n",
    "    # Read the JSONL file\n",
    "    queries = []\n",
    "    with open(evaluation_queries_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    query_data = json.loads(line)\n",
    "                    if 'query' in query_data:\n",
    "                        queries.append(query_data['query'])\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Warning: Line {line_num} missing 'query' field: {line}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö†Ô∏è  Warning: Invalid JSON on line {line_num}: {line}\")\n",
    "                    print(f\"   Error: {str(e)}\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded {len(queries)} evaluation queries\")\n",
    "    \n",
    "    # Analyze the types of queries\n",
    "    azure_ai_queries = [q for q in queries if any(keyword in q.lower() for keyword in ['azure', 'ai', 'openai', 'cognitive', 'machine learning'])]\n",
    "    general_queries = [q for q in queries if q not in azure_ai_queries]\n",
    "    \n",
    "    print(f\"üìä Query breakdown:\")\n",
    "    print(f\"   ‚Ä¢ Azure AI related queries: {len(azure_ai_queries)}\")\n",
    "    print(f\"   ‚Ä¢ General/off-topic queries: {len(general_queries)}\")\n",
    "    print(f\"   ‚Ä¢ Total queries: {len(queries)}\")\n",
    "    \n",
    "    # Show some sample queries\n",
    "    print(f\"\\nüìù Sample Azure AI queries:\")\n",
    "    for i, query in enumerate(azure_ai_queries[:3], 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    print(f\"\\nüìù Sample general queries:\")\n",
    "    for i, query in enumerate(general_queries[:3], 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading evaluation queries: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d1260",
   "metadata": {},
   "source": [
    "## Run Comprehensive Agent Evaluation\n",
    "\n",
    "Execute the evaluation pipeline using all configured evaluators against the test dataset. This process will measure agent performance across multiple dimensions.\n",
    "\n",
    "**Note**: This evaluation may take a significant amount of time depending on the number of queries and the response time of the agent and evaluation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65deb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for evaluation\n",
    "evaluation_name = f\"questionnaire_agent_evaluation_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üöÄ Starting comprehensive evaluation: {evaluation_name}\")\n",
    "print(f\"üìä Total queries to evaluate: {len(queries)}\")\n",
    "print(f\"‚è∞ Started at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚ö†Ô∏è  This evaluation may take 10-30 minutes depending on query complexity...\")\n",
    "print(\"   Each query requires multiple API calls to the agent and evaluation models.\")\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    # Configure the evaluators dictionary with proper naming for Azure AI Foundry\n",
    "    evaluators_config = {\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "    }\n",
    "    \n",
    "    # Execute the evaluation\n",
    "    evaluation_result = evaluate(\n",
    "        data=evaluation_queries_file,\n",
    "        target=query_questionnaire_agent,\n",
    "        evaluators=evaluators_config,\n",
    "        azure_ai_project=AZURE_OPENAI_ENDPOINT,  # Azure AI project endpoint\n",
    "        evaluation_name=evaluation_name,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation completed successfully!\")\n",
    "    print(f\"‚è∞ Finished at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Display Azure AI Foundry Studio URL if available\n",
    "    if 'studio_url' in evaluation_result:\n",
    "        print(f\"üîó Azure AI Foundry Studio URL: {evaluation_result['studio_url']}\")\n",
    "        print(\"   You can view detailed results and visualizations in the Azure AI Foundry portal.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {str(e)}\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   ‚Ä¢ Ensure Azure CLI authentication is working: 'az login'\")\n",
    "    print(\"   ‚Ä¢ Check that all required environment variables are set\")\n",
    "    print(\"   ‚Ä¢ Verify Azure OpenAI API quotas and limits\")\n",
    "    print(\"   ‚Ä¢ Check network connectivity to Azure services\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54140a",
   "metadata": {},
   "source": [
    "## Process and Display Evaluation Results\n",
    "\n",
    "Extract and format evaluation metrics, display summary statistics, and provide detailed analysis of agent performance across different query types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display evaluation metrics\n",
    "print(\"üìä EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get the overall metrics\n",
    "    metrics = evaluation_result.get(\"metrics\", {})\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"‚ö†Ô∏è  No metrics found in evaluation results\")\n",
    "        print(\"   This might indicate an issue with the evaluation process\")\n",
    "    else:\n",
    "        print(\"\\nüéØ Overall Performance Metrics:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            # Format the metric value based on its type\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value}\")\n",
    "        \n",
    "        # Calculate and display average score\n",
    "        numeric_metrics = {k: v for k, v in metrics.items() if isinstance(v, (int, float))}\n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            print(f\"\\nüìà Average Score Across All Metrics: {avg_score:.4f}\")\n",
    "            \n",
    "            # Provide interpretation\n",
    "            print(f\"\\nüìù Performance Interpretation:\")\n",
    "            if avg_score >= 4.0:\n",
    "                print(\"   üü¢ Excellent: The agent demonstrates high-quality responses\")\n",
    "            elif avg_score >= 3.0:\n",
    "                print(\"   üü° Good: The agent performs well with room for improvement\")\n",
    "            elif avg_score >= 2.0:\n",
    "                print(\"   üü† Fair: The agent shows moderate performance, needs improvement\")\n",
    "            else:\n",
    "                print(\"   üî¥ Poor: The agent requires significant improvements\")\n",
    "        \n",
    "        # Analyze individual metrics\n",
    "        print(f\"\\nüîç Detailed Metric Analysis:\")\n",
    "        for metric_name, metric_value in numeric_metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                if metric_value >= 4.0:\n",
    "                    status = \"üü¢ Excellent\"\n",
    "                elif metric_value >= 3.0:\n",
    "                    status = \"üü° Good\"\n",
    "                elif metric_value >= 2.0:\n",
    "                    status = \"üü† Fair\"\n",
    "                else:\n",
    "                    status = \"üî¥ Poor\"\n",
    "                print(f\"   ‚Ä¢ {metric_name.title()}: {metric_value:.4f} - {status}\")\n",
    "    \n",
    "    # Display additional result information\n",
    "    if 'outputs' in evaluation_result:\n",
    "        outputs = evaluation_result['outputs']\n",
    "        print(f\"\\nüìã Detailed Results Available:\")\n",
    "        print(f\"   ‚Ä¢ Number of evaluated samples: {len(outputs) if hasattr(outputs, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    # Display data info if available\n",
    "    if 'data' in evaluation_result:\n",
    "        print(f\"\\nüìÅ Evaluation Data Info:\")\n",
    "        print(f\"   ‚Ä¢ Data source: {evaluation_queries_file}\")\n",
    "        print(f\"   ‚Ä¢ Total queries processed: {len(queries)}\")\n",
    "        print(f\"   ‚Ä¢ Azure AI queries: {len(azure_ai_queries)}\")\n",
    "        print(f\"   ‚Ä¢ General queries: {len(general_queries)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing evaluation results: {str(e)}\")\n",
    "    print(\"   Raw evaluation result keys:\", list(evaluation_result.keys()) if evaluation_result else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a51bf",
   "metadata": {},
   "source": [
    "## Export Results for Analysis\n",
    "\n",
    "Save evaluation results to files, generate reports, and provide information for further analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41697c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation results to files\n",
    "results_dir = Path(\"evaluation_results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "summary_file = results_dir / f\"evaluation_summary_{timestamp}.txt\"\n",
    "\n",
    "try:\n",
    "    # Save detailed results as JSON\n",
    "    print(f\"üíæ Saving detailed results to: {results_file}\")\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"evaluation_name\": evaluation_name,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"configuration\": {\n",
    "                \"model_deployment\": AZURE_OPENAI_MODEL_DEPLOYMENT,\n",
    "                \"evaluation_model\": EVALUATION_MODEL,\n",
    "                \"total_queries\": len(queries),\n",
    "                \"azure_ai_queries\": len(azure_ai_queries),\n",
    "                \"general_queries\": len(general_queries)\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"evaluation_result\": evaluation_result\n",
    "        }, f, indent=2, default=str)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"üìÑ Generating summary report: {summary_file}\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"QUESTIONNAIRE AGENT EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Evaluation Name: {evaluation_name}\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Queries Evaluated: {len(queries)}\\n\")\n",
    "        f.write(f\"Azure AI Related Queries: {len(azure_ai_queries)}\\n\")\n",
    "        f.write(f\"General/Off-topic Queries: {len(general_queries)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                f.write(f\"{metric_name.title()}: {metric_value:.4f}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{metric_name.title()}: {metric_value}\\n\")\n",
    "        \n",
    "        if numeric_metrics:\n",
    "            avg_score = sum(numeric_metrics.values()) / len(numeric_metrics)\n",
    "            f.write(f\"\\nAverage Score: {avg_score:.4f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nPerformance Level: \")\n",
    "            if avg_score >= 4.0:\n",
    "                f.write(\"Excellent\\n\")\n",
    "            elif avg_score >= 3.0:\n",
    "                f.write(\"Good\\n\")\n",
    "            elif avg_score >= 2.0:\n",
    "                f.write(\"Fair\\n\")\n",
    "            else:\n",
    "                f.write(\"Poor\\n\")\n",
    "        \n",
    "        if 'studio_url' in evaluation_result:\n",
    "            f.write(f\"\\nAzure AI Foundry Studio URL:\\n{evaluation_result['studio_url']}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Results exported successfully!\")\n",
    "    print(f\"üìÅ Results directory: {results_dir.absolute()}\")\n",
    "    print(f\"   ‚Ä¢ Detailed JSON: {results_file.name}\")\n",
    "    print(f\"   ‚Ä¢ Summary report: {summary_file.name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exporting results: {str(e)}\")\n",
    "    print(\"   Results are still available in the evaluation_result variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e41476",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This evaluation notebook provides a comprehensive assessment of the Questionnaire Agent's performance across multiple dimensions:\n",
    "\n",
    "### What This Evaluation Measures:\n",
    "\n",
    "1. **Groundedness**: How well the agent's responses are based on reliable sources and context\n",
    "2. **Relevance**: How relevant and on-topic the responses are to the input queries\n",
    "3. **Coherence**: How logically structured and internally consistent the responses are\n",
    "4. **Fluency**: How well-written, readable, and linguistically sound the responses are\n",
    "\n",
    "### Evaluation Dataset:\n",
    "\n",
    "- **Azure AI Queries**: Tests the agent's knowledge and accuracy on its primary domain\n",
    "- **General Queries**: Tests the agent's ability to stay on topic and handle off-domain questions appropriately\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- **Objective Assessment**: Uses AI-assisted evaluation for consistent, scalable assessment\n",
    "- **Multi-dimensional Analysis**: Evaluates different aspects of response quality\n",
    "- **Azure Integration**: Results are available in Azure AI Foundry Studio for detailed analysis\n",
    "- **Reproducible**: Can be run repeatedly to track improvements over time\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review Detailed Results**: Use the Azure AI Foundry Studio URL to explore individual query results\n",
    "2. **Identify Improvement Areas**: Focus on metrics with lower scores\n",
    "3. **Iterative Development**: Re-run evaluation after making improvements to track progress\n",
    "4. **Expand Evaluation**: Consider adding more specific test cases or custom evaluators\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "- Detailed JSON results for programmatic analysis\n",
    "- Human-readable summary reports\n",
    "- Links to Azure AI Foundry Studio for interactive exploration"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
